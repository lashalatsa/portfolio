{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DtNBZFHO3M7n"
   },
   "source": [
    "# **Automatidata project**\n",
    "**Course 6 - The Nuts and bolts of machine learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ttxbfHXzB4e"
   },
   "source": [
    "You are a data professional in a data analytics firm called Automatidata. Their client, the New York City Taxi & Limousine Commission (New York City TLC), was impressed with the work you have done and has requested that you **build a machine learning model to predict if a customer will not leave a tip**. They want to use the model in an app that will alert taxi drivers to customers who are unlikely to tip, since drivers depend on tips, and the ability to filter out people who don't tip would help increase driver revenue. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgSbVJvomcVa"
   },
   "source": [
    "# Course 6 End-of-course project: Build a machine learning model\n",
    "\n",
    "In this activity, you will practice using tree-based modeling techniques to predict on a binary target class.  \n",
    "<br/>   \n",
    "\n",
    "**The purpose** of this model is to find ways to generate more revenue for taxi cab drivers.  \n",
    "  \n",
    "**The goal** of this model is to predict whether or not a customer is a generous tipper.  \n",
    "<br/>  \n",
    "\n",
    "*This activity has three parts:*\n",
    "\n",
    "**Part 1:** Ethical considerations \n",
    "* Consider the ethical implications of the request \n",
    "\n",
    "* Should the objective of the model be adjusted?\n",
    "\n",
    "**Part 2:** Feature engineering\n",
    "\n",
    "* Perform feature selection, extraction, and transformation to prepare the data for modeling\n",
    "\n",
    "**Part 3:** Modeling\n",
    "\n",
    "* Build the models, evaluate them, and advise on next steps\n",
    "\n",
    "Follow the instructions and answer the questions below to complete the activity. Then, complete an Executive Summary using the questions listed on the PACE Strategy Document. \n",
    "\n",
    "Be sure to complete this activity before moving on. The next course item will provide you with a completed exemplar to compare to your own work. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that you have a helpful tool at your disposal! Refer to the [PACE Strategy Document](https://docs.google.com/document/d/1hPtIs4X7c5xmLSi8qs7Og2FEQHkELXBC_pGuJI1jF9o/template/preview?resourcekey=0-mSL0tC7opaF8XIOdXa1JIw) to apply your learnings, apply new problem-solving skills, and guide your approach to this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NzDjfCSLf6Jq"
   },
   "source": [
    "# **PACE stages** \n",
    "\n",
    "\n",
    "<img src=\"images/Pace.png\" width=\"100\" height=\"100\" align=left>\n",
    "\n",
    "   *        [Plan](#scrollTo=psz51YkZVwtN&line=3&uniqifier=1)\n",
    "   *        [Analyze](#scrollTo=mA7Mz_SnI8km&line=4&uniqifier=1)\n",
    "   *        [Construct](#scrollTo=Lca9c8XON8lc&line=2&uniqifier=1)\n",
    "   *        [Execute](#scrollTo=401PgchTPr4E&line=2&uniqifier=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==> ENTER YOUR RESPONSES TO QUESTIONS 1-5 HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GUUrVKTe4cc5"
   },
   "source": [
    "Suppose you were to modify the modeling objective so, instead of predicting people who won't tip at all, you predicted people who are particularly generous&mdash;those who will tip 20% or more? Consider the following questions:\n",
    "\n",
    "1.  **What features do you need to make this prediction?**  \n",
    "\n",
    "2.  **What would be the target variable?**  \n",
    "\n",
    "3.  **What metric should you use to evaluate your model? Do you have enough information to decide this now?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==> ENTER YOUR RESPONSES TO QUESTIONS 1-3 HERE\n",
    "1. we have behavioral history for each customer, so we could know how much they tipped on previous taxi rides. We'd also want times, dates, and locations of both pickups and dropoffs, estimated fares, and payment method.\n",
    "\n",
    "2.The target variable would be a binary variable (1 or 0) that indicates whether or not the customer is expected to tip ≥ 20%\n",
    "\n",
    "3.This is a supervised learning, classification task. We could use accuracy, precision, recall, F-score, area under the ROC curve, or a number of other metrics. However, we don't have enough information at this time to know which are most appropriate. We need to know the class balance of the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**_Complete the following steps to begin:_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8Vm3QEfGELS"
   },
   "source": [
    "### **Task 1. Imports and data loading**\n",
    "\n",
    "Import packages and libraries needed to build and evaluate random forest and XGBoost classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "fKhnX2Puf4Bt"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This displays all of the columns, preventing Juptyer from redacting them.\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score,\\\n",
    "f1_score, confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# This is the function that helps plot feature importance \n",
    "from xgboost import plot_importance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IeXTZ2tdbALL"
   },
   "source": [
    "`Pandas` reads in the dataset as `df0`, now inspect the first five rows. As shown in this cell, the dataset has been automatically loaded in for you. You do not need to download the .csv file, or provide more code, in order to access the dataset and proceed with this lab. Please continue with this activity by completing the following instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "5weTXGKqa_iG"
   },
   "outputs": [],
   "source": [
    "#==> Read in data\n",
    "df0 = pd.read_csv('2017_Yellow_Taxi_Trip_Data.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EgPRBjizg1oo"
   },
   "source": [
    "<img src=\"images/Analyze.png\" width=\"100\" height=\"100\" align=left>\n",
    "\n",
    "## PACE: **Analyze**\n",
    "\n",
    "Consider the questions in your PACE Strategy Documentto reflect on the Analyze stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VZowX9rhU1o"
   },
   "source": [
    "### **Task 2. Feature engineering**\n",
    "\n",
    "You have already prepared much of this data and performed exploratory data analysis (EDA) in previous courses. \n",
    "\n",
    "Call `info()` on the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "mBOSW8IDbO_d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22699 entries, 0 to 22698\n",
      "Data columns (total 18 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   Unnamed: 0             22699 non-null  int64  \n",
      " 1   VendorID               22699 non-null  int64  \n",
      " 2   tpep_pickup_datetime   22699 non-null  object \n",
      " 3   tpep_dropoff_datetime  22699 non-null  object \n",
      " 4   passenger_count        22699 non-null  int64  \n",
      " 5   trip_distance          22699 non-null  float64\n",
      " 6   RatecodeID             22699 non-null  int64  \n",
      " 7   store_and_fwd_flag     22699 non-null  object \n",
      " 8   PULocationID           22699 non-null  int64  \n",
      " 9   DOLocationID           22699 non-null  int64  \n",
      " 10  payment_type           22699 non-null  int64  \n",
      " 11  fare_amount            22699 non-null  float64\n",
      " 12  extra                  22699 non-null  float64\n",
      " 13  mta_tax                22699 non-null  float64\n",
      " 14  tip_amount             22699 non-null  float64\n",
      " 15  tolls_amount           22699 non-null  float64\n",
      " 16  improvement_surcharge  22699 non-null  float64\n",
      " 17  total_amount           22699 non-null  float64\n",
      "dtypes: float64(8), int64(7), object(3)\n",
      "memory usage: 3.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df0.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0D2RvXk0kwsx"
   },
   "source": [
    "You know from your EDA that customers who pay cash generally have a tip amount of $0. To meet the modeling objective, you'll need to sample the data to select only the customers who pay with credit card. \n",
    "\n",
    "Copy `df0` and assign the result to a variable called `df`. Then, use a Boolean mask to filter `df1` so it contains only customers who paid with credit card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "_pmNd78plQYr"
   },
   "outputs": [],
   "source": [
    "# Subset the data to isolate only customers who paid by credit card\n",
    "\n",
    "# Copy df0 and assign to df1\n",
    "df1 = df0.copy() \n",
    "\n",
    "# Filter df1 to contain only credit card customers\n",
    "df1 = df1[df1['payment_type']==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EcYudtSYyMcZ"
   },
   "source": [
    "##### **Target**\n",
    "\n",
    "Notice that there isn't a column that indicates tip percent, which is what you need to create the target variable. You'll have to engineer it. \n",
    "\n",
    "Add a `tip_percent` column to the dataframe by performing the following calculation:  \n",
    "<br/>  \n",
    "\n",
    "\n",
    "$$tip\\ percent = \\frac{tip\\ amount}{total\\ amount - tip\\ amount}$$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "guanzJd8zBla"
   },
   "outputs": [],
   "source": [
    "# Create tip % col\n",
    "df1['tip_percent'] = df1['tip_amount'] / (df1['total_amount'] - df1['tip_amount'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bqb-SWfs-8Xn"
   },
   "source": [
    "Now create another column called `generous`. This will be the target variable. The column should be a binary indicator of whether or not a customer tipped ≥ 20% (0=no, 1=yes).\n",
    "\n",
    "1. Begin by making the `generous` column a copy of the `tip_percent` column.\n",
    "2. Reassign the column by converting it to Boolean (True/False).\n",
    "3. Reassign the column by converting Boolean to binary (1/0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "nqDSe0DSGwhB"
   },
   "outputs": [],
   "source": [
    "# Create 'generous' col (target)\n",
    "df1['generous'] = df1['tip_percent']\n",
    "df1['generous'] = (df1['generous'] >= 0.2)\n",
    "df1['generous'] = df1['generous'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddLE6KE1KeF7"
   },
   "source": [
    "<details>\n",
    "  <summary><h5>HINT</h5></summary>\n",
    "\n",
    "To convert from Boolean to binary, use `.astype(int)` on the column.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EkubbZRRKrjO"
   },
   "source": [
    "##### **Features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xa7WNUMXiHNk"
   },
   "source": [
    "Which columns are obviously unpredictive of tip percentage? Refer to the data dictionary.\n",
    "\n",
    "Drop `Unnamed: 0` and `store_and_fwd_flag` columns. Assign the result back to `df1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "-sAKB7EqiGBs"
   },
   "outputs": [],
   "source": [
    "drop_cols = ['Unnamed: 0', 'store_and_fwd_flag']\n",
    "df1 = df1.drop(drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H27zUVIlkaxA"
   },
   "source": [
    "Next, you're going to be working with the pickup and dropoff columns. To do this, you'll need to import the `datetime` module. Import this module as `dt`. \n",
    "\n",
    "Then, convert the `tpep_pickup_datetime` and `tpep_dropoff_datetime` columns to the datetime class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "OIycxWBMkafJ"
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "# Convert pickup and dropoff cols to datetime\n",
    "df1[['tpep_pickup_datetime', 'tpep_dropoff_datetime']] = df1[['tpep_pickup_datetime', 'tpep_dropoff_datetime']].apply(pd.to_datetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usqkkmG1o1Wy"
   },
   "source": [
    "\n",
    "Create a new column called `duration`, which captures the time elapsed from pickup to dropoff.\n",
    "\n",
    "1.  Subtract `tpep_pickup_datetime` from `tpep_dropoff_datetime` and assign the result to a new column called `duration`.\n",
    "2.  Convert the `duration` column to seconds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "JxibhpbJkWgK"
   },
   "outputs": [],
   "source": [
    "# Create ride 'duration' col\n",
    "df1['duration'] = df1['tpep_dropoff_datetime'] - df1['tpep_pickup_datetime']\n",
    "\n",
    "# Convert 'duration' col to seconds\n",
    "df1['duration'] = df1['duration'].dt.total_seconds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-S-w5lPQZQk"
   },
   "source": [
    "\n",
    "<details>\n",
    "  <summary><h5>HINT</h5></summary>\n",
    "\n",
    "To convert to seconds, use `dt.total_seconds()` on the column.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zpcM4FvNyPFY"
   },
   "source": [
    "Create a `day` column that contains only the day of the week when each passenger was picked up. Then, convert the values to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "abUvtMaYyWpD"
   },
   "outputs": [],
   "source": [
    "df1['day'] = df1['tpep_pickup_datetime'].dt.day_name().str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RZZhKnQrQgNM"
   },
   "source": [
    "\n",
    "<details>\n",
    "  <summary><h5>HINT</h5></summary>\n",
    "\n",
    "To convert to day name, use `dt.day_name()` on the column.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwslVt8Hpu7x"
   },
   "source": [
    "Next, engineer four new columns that represent time of day bins. Each column should contain binary values (0=no, 1=yes) that indicate whether a trip began (picked up) during the following times:\n",
    "\n",
    "`am_rush` = [06:00&ndash;10:00)  \n",
    "`daytime` = [10:00&ndash;16:00)  \n",
    "`pm_rush` = [16:00&ndash;20:00)  \n",
    "`nighttime` = [20:00&ndash;06:00)\n",
    "\n",
    "To do this, first create the four columns. For now, each new column should contain the same information: the hour (only) from the `tpep_pickup_datetime` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "x8LFySUyprau"
   },
   "outputs": [],
   "source": [
    "# Create 'am_rush' col\n",
    "df1['am_rush'] = df1['tpep_pickup_datetime'].dt.hour\n",
    "\n",
    "# Create 'daytime' col\n",
    "df1['daytime'] = df1['tpep_pickup_datetime'].dt.hour\n",
    "\n",
    "# Create 'pm_rush' col\n",
    "df1['pm_rush'] = df1['tpep_pickup_datetime'].dt.hour\n",
    "\n",
    "# Create 'nighttime' col\n",
    "df1['nighttime'] = df1['tpep_pickup_datetime'].dt.hour\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HDyfsTDvwORL"
   },
   "source": [
    "You'll need to write four functions to convert each new column to binary (0/1). Begin with `am_rush`. Complete the function so if the hour is between [06:00–10:00), it returns 1, otherwise, it returns 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "oAE4vRz0wGtN"
   },
   "outputs": [],
   "source": [
    "# Define 'am_rush()' conversion function [06:00–10:00)\n",
    "def am_rush(hour):\n",
    "    if 6 <= hour['am_rush'] < 10:\n",
    "        val = 1\n",
    "    else:\n",
    "        val = 0\n",
    "    return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hHY1-6cIxfA6"
   },
   "source": [
    "Now, apply the `am_rush()` function to the `am_rush` series to perform the conversion. Print the first five values of the column to make sure it did what you expected it to do.\n",
    "\n",
    "**Note:** Be careful! If you run this cell twice, the function will be reapplied and the values will all be changed to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "sWFojyk9xdDY"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    0\n",
       "2    1\n",
       "3    0\n",
       "5    0\n",
       "Name: am_rush, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply 'am_rush' function to the 'am_rush' series\n",
    "df1['am_rush'] = df1.apply(am_rush, axis=1)\n",
    "df1['am_rush'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mSY6SsdK0lpn"
   },
   "source": [
    "Write functions to convert the three remaining columns and apply them to their respective series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "UADnzaIjzwLG"
   },
   "outputs": [],
   "source": [
    "# Define 'daytime()' conversion function [10:00–16:00)\n",
    "def daytime(hour):\n",
    "    if 10 <= hour['daytime'] < 16:\n",
    "        val = 1\n",
    "    else:\n",
    "        val = 0\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "ReHpKxoC1Qsx"
   },
   "outputs": [],
   "source": [
    "# Apply 'daytime()' function to the 'daytime' series\n",
    "df1['daytime'] = df1.apply(daytime, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "rP-ZBOHT1WQY"
   },
   "outputs": [],
   "source": [
    "# Define 'pm_rush()' conversion function [16:00–20:00)\n",
    "def pm_rush(hour):\n",
    "    if 16 <= hour['pm_rush'] < 20:\n",
    "        val = 1\n",
    "    else:\n",
    "        val = 0\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "h0zWPBqr1mX4"
   },
   "outputs": [],
   "source": [
    "# Apply 'pm_rush' function to the 'pm_rush' series\n",
    "df1['pm_rush'] = df1.apply(pm_rush, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "u5O0LPLz2CSa"
   },
   "outputs": [],
   "source": [
    "# Define 'nighttime()' conversion function [20:00–06:00)\n",
    "def nighttime(hour):\n",
    "    if 20 <= hour['nighttime'] < 24:\n",
    "        val = 1\n",
    "    elif 0 <= hour['nighttime'] < 6:\n",
    "        val = 1\n",
    "    else:\n",
    "        val = 0\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "kLGmBXkT2RTi"
   },
   "outputs": [],
   "source": [
    "# Apply 'nighttime' function to the 'nighttime' series\n",
    "df1['nighttime'] = df1.apply(nighttime, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VrUmDy8U28bs"
   },
   "source": [
    "Now, create a `month` column that contains only the abbreviated name of the month when each passenger was picked up, then convert the result to lowercase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bU5Zchdxgk3w"
   },
   "source": [
    "<details>\n",
    "  <summary><h5>HINT</h5></summary>\n",
    "\n",
    "Refer to the [strftime cheatsheet](https://strftime.org/) for help.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "rv5ZKK6-2YAh"
   },
   "outputs": [],
   "source": [
    "# Create 'month' col\n",
    "df1['month'] = df1['tpep_pickup_datetime'].dt.strftime('%b').str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKdaB3tG4dNB"
   },
   "source": [
    "Because you have encoded much of the information contained in the pickup and dropoff columns into new columns, you can drop them for modeling. \n",
    "\n",
    "1. Drop the `tpep_pickup_datetime` and `tpep_dropoff_datetime` columns and reassign the result back to `df1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "OMG92slC4bOv"
   },
   "outputs": [],
   "source": [
    "df1 = df1.drop(['tpep_pickup_datetime',\t'tpep_dropoff_datetime'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qWbNVbngihE6"
   },
   "source": [
    "Examine the first five rows of your dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "jWxemeyl4vwQ"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>tip_percent</th>\n",
       "      <th>generous</th>\n",
       "      <th>duration</th>\n",
       "      <th>day</th>\n",
       "      <th>am_rush</th>\n",
       "      <th>daytime</th>\n",
       "      <th>pm_rush</th>\n",
       "      <th>nighttime</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3.34</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>231</td>\n",
       "      <td>1</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>16.56</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>844.0</td>\n",
       "      <td>saturday</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>mar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1</td>\n",
       "      <td>186</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>20.80</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>1</td>\n",
       "      <td>1590.0</td>\n",
       "      <td>tuesday</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>apr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>262</td>\n",
       "      <td>236</td>\n",
       "      <td>1</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>8.75</td>\n",
       "      <td>0.198630</td>\n",
       "      <td>0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>friday</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3.70</td>\n",
       "      <td>1</td>\n",
       "      <td>188</td>\n",
       "      <td>97</td>\n",
       "      <td>1</td>\n",
       "      <td>20.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>6.39</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>27.69</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1</td>\n",
       "      <td>1815.0</td>\n",
       "      <td>sunday</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>may</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2.30</td>\n",
       "      <td>1</td>\n",
       "      <td>161</td>\n",
       "      <td>236</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>12.36</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>480.0</td>\n",
       "      <td>saturday</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>mar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VendorID  passenger_count  trip_distance  RatecodeID  PULocationID  \\\n",
       "0         2                6           3.34           1           100   \n",
       "1         1                1           1.80           1           186   \n",
       "2         1                1           1.00           1           262   \n",
       "3         2                1           3.70           1           188   \n",
       "5         2                6           2.30           1           161   \n",
       "\n",
       "   DOLocationID  payment_type  fare_amount  extra  mta_tax  tip_amount  \\\n",
       "0           231             1         13.0    0.0      0.5        2.76   \n",
       "1            43             1         16.0    0.0      0.5        4.00   \n",
       "2           236             1          6.5    0.0      0.5        1.45   \n",
       "3            97             1         20.5    0.0      0.5        6.39   \n",
       "5           236             1          9.0    0.5      0.5        2.06   \n",
       "\n",
       "   tolls_amount  improvement_surcharge  total_amount  tip_percent  generous  \\\n",
       "0           0.0                    0.3         16.56     0.200000         1   \n",
       "1           0.0                    0.3         20.80     0.238095         1   \n",
       "2           0.0                    0.3          8.75     0.198630         0   \n",
       "3           0.0                    0.3         27.69     0.300000         1   \n",
       "5           0.0                    0.3         12.36     0.200000         1   \n",
       "\n",
       "   duration       day  am_rush  daytime  pm_rush  nighttime month  \n",
       "0     844.0  saturday        1        0        0          0   mar  \n",
       "1    1590.0   tuesday        0        1        0          0   apr  \n",
       "2     432.0    friday        1        0        0          0   dec  \n",
       "3    1815.0    sunday        0        1        0          0   may  \n",
       "5     480.0  saturday        0        0        0          1   mar  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVs01W-Iitu7"
   },
   "source": [
    "Many of the columns are categorical and will need to be dummied (converted to binary). Some of these columns are numeric, but they actually encode categorical information, such as `RatecodeID` and the pickup and dropoff locations. To make these columns recognizable to the `get_dummies()` function as categorical variables, you'll first need to convert them to `type(str)`. \n",
    "\n",
    "1. Define a variable called `cols_to_str`, which is a list of the numeric columns that contain categorical information and must be converted to string: `RatecodeID`, `PULocationID`, `DOLocationID`.\n",
    "2. Write a for loop that converts each column in `cols_to_string` to string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "FbB4AfATHqjC"
   },
   "outputs": [],
   "source": [
    "cols_to_str = ['RatecodeID',\t'PULocationID',\t'DOLocationID']\n",
    "\n",
    "for col in cols_to_str:\n",
    "    df1[col] = df1[col].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6j6Nyb5RnsvC"
   },
   "source": [
    "\n",
    "<details>\n",
    "  <summary><h5>HINT</h5></summary>\n",
    "\n",
    "To convert to string, use `astype(str)` on the column.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y15DMa1mn7Dl"
   },
   "source": [
    "The `VendorID` column is also a numerical column that contains categorical information (which taxi cab company picked up the passenger). The values are all 1 or 2. \n",
    "\n",
    "1. Convert this to binary by subtracting 1 from every value in the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "lDjgYjYhHqhZ"
   },
   "outputs": [],
   "source": [
    "# Subtract 1 from every value in 'VendorID' col\n",
    "df1['VendorID'] = df1['VendorID'] - 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_5Ubw8O1pKRO"
   },
   "source": [
    "Now convert all the categorical columns to binary.\n",
    "\n",
    "1. Call `get_dummies()` on the dataframe and assign the results back to a new dataframe called `df2`. Don't use the `drop_first` parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "H94yLzUMHqgB"
   },
   "outputs": [],
   "source": [
    "df2 = pd.get_dummies(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e06_pKh1qUYz"
   },
   "source": [
    "Finally, drop the columns that are constant or that contain information that would be a proxy for our target variable. For example, `total_amount` contains tip amount, and therefore tip percentage, if used with `fare_amount`. And `mta_tax` is $0.50 99.6% of the time, so it's not adding any predictive signal to the model.\n",
    "\n",
    "1. Drop the following features: `payment_type`, `mta_tax`, `tip_amount`, `total_amount`, and `tip_percent`. Assign the results to a new dataframe called `df3`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "7ZdchaCj4OGm"
   },
   "outputs": [],
   "source": [
    "df3 = df2.drop(['payment_type', 'mta_tax', 'tip_amount', 'total_amount', 'tip_percent'],\n",
    "               axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nZfNE37b-LlJ"
   },
   "source": [
    "##### **Evaluation metric**\n",
    "\n",
    "Before modeling, you must decide on an evaluation metric. \n",
    "\n",
    "1. Examine the class balance of your target variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "4mRefXCF-K_c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    9944\n",
       "1    5321\n",
       "Name: generous, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3['generous'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TjgkLrOf_OrE"
   },
   "source": [
    "Approximately 1/3 of the customers in this dataset were \"generous\" (tipped ≥ 20%). The dataset is imbalanced, but not extremely so. \n",
    "\n",
    "To determine a metric, consider the cost of both kinds of model error:\n",
    "* False positives (the model predicts a tip ≥ 20%, but the customer does not give one)\n",
    "* False negatives (the model predicts a tip < 20%, but the customer gives more)\n",
    "\n",
    "False positives are worse for cab drivers, because they would pick up a customer expecting a good tip and then not receiving one.\n",
    "\n",
    "False negatives are worse for customers, because a cab driver would likely pick up a different customer who was predicted to tip more.\n",
    "\n",
    "**Since your client represents taxi drivers, use a metric that evaluates false positives. Which metric is this?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision is the metric that measures true positives, and therefore, by definition, also false positives. It calculates the percentage of your model's predicted responders that were actual responders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3n1eikFh8akS"
   },
   "source": [
    "<img src=\"images/Construct.png\" width=\"100\" height=\"100\" align=left>\n",
    "\n",
    "## PACE: **Construct**\n",
    "\n",
    "Consider the questions in your PACE Strategy Document to reflect on the Construct stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5jzGjOS8iiv"
   },
   "source": [
    "### **Task 3. Modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nx41bVxX89Fe"
   },
   "source": [
    "##### **Split the data**\n",
    "\n",
    "Now you're ready to model. The only remaining step is to split the data into features/target variable and training/testing data. \n",
    "\n",
    "1. Define a variable `y` that isolates the target variable (`generous`).\n",
    "2. Define a variable `X` that isolates the features.\n",
    "3. Split the data into training and testing sets. Put 20% of the samples into the test set, stratify the data, and set the random state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "qLbapbSWDUL-"
   },
   "outputs": [],
   "source": [
    "y = df3['generous']\n",
    "X = df3.drop('generous', axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vynZs5het1b_"
   },
   "source": [
    "##### **Random forest**\n",
    "\n",
    "Begin with using `GridSearchCV` to tune a random forest model.\n",
    "\n",
    "1. Instantiate the random forest classifier `rf` and set the random state.\n",
    "\n",
    "2. Create a dictionary `cv_params` of any of the following hyperparameters and their corresponding values to tune. The more you tune, the better your model will fit the data, but the longer it will take. \n",
    " - `max_depth`  \n",
    " - `max_features`  \n",
    " - `max_samples` \n",
    " - `min_samples_leaf`  \n",
    " - `min_samples_split`\n",
    " - `n_estimators`  \n",
    "\n",
    "3. Define a dictionary `scoring` of scoring metrics for GridSearch to capture (precision, recall, F1 score, and accuracy).\n",
    "\n",
    "4. Instantiate the `GridSearchCV` object `rf_cv1`. Pass to it as arguments:\n",
    " - estimator=`rf`\n",
    " - param_grid=`cv_params`\n",
    " - scoring=`scoring`\n",
    " - cv: define the number of you cross-validation folds you want (`cv=_`)\n",
    " - refit: indicate which evaluation metric you want to use to select the model (`refit=_`)\n",
    "\n",
    "\n",
    "**Note:** `refit` should be set to `'precision'`.<font/>\n",
    "</details>\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "Vj5rJWOv5O3d"
   },
   "outputs": [],
   "source": [
    "# 1. Instantiate the random forest classifier\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "\n",
    "# 2. Create a dictionary of hyperparameters to tune\n",
    "cv_params = {'max_depth': [3,5, None], \n",
    "             'max_features': [1.0],\n",
    "             'max_samples': [0.7, 1.0],\n",
    "             'min_samples_leaf': [1,2,3],\n",
    "             'min_samples_split': [2,3,4],\n",
    "             'n_estimators': [300, 500],\n",
    "             }  \n",
    "\n",
    "# 3. Define a dictionary of scoring metrics to capture\n",
    "scoring = {'accuracy', 'precision', 'recall', 'f1'}\n",
    "\n",
    "# 4. Instantiate the GridSearchCV object\n",
    "rf_cv1 = GridSearchCV(rf, cv_params, scoring=scoring, cv=4, refit='precision')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wv_WvRA1RqTl"
   },
   "source": [
    "Now fit the model to the training data.\n",
    "\n",
    "**_Note_**: _The following operation may take over an hour to complete_. Therefore, the cell has been commented out along with code cell #33 (where we pickle the model). To save time, you can skip these cells and continue to execute the cells in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "OXuBiTGi5ZHn"
   },
   "outputs": [],
   "source": [
    "rf_cv1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5wHi_YJduQOH"
   },
   "source": [
    "<details>\n",
    "  <summary><h5>HINT</h5></summary>\n",
    "\n",
    "If you get a warning that a metric is 0 due to no predicted samples, think about how many features you're sampling with `max_features`. How many features are in the dataset? How many are likely predictive enough to give good predictions within the number of splits you've allowed (determined by the `max_depth` hyperparameter)? Consider increasing `max_features`.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QIaRiZW4hf-6"
   },
   "source": [
    "Examine the best average score across all the validation folds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "29kGUegqhviL"
   },
   "outputs": [],
   "source": [
    "rf_cv1.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "heGb51fHh3E5"
   },
   "source": [
    "Examine the best combination of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FjgXbO7Kh8is"
   },
   "outputs": [],
   "source": [
    "rf_cv1.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZZnem5yiAau"
   },
   "source": [
    "Use the `make_results()` function to output all of the scores of your model. Note that it accepts three arguments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GeW48TS742jN"
   },
   "source": [
    "<details>\n",
    "  <summary><h5>HINT</h5></summary>\n",
    "\n",
    "To learn more about how this function accesses the cross-validation results, refer to the [`GridSearchCV` scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html?highlight=gridsearchcv#sklearn.model_selection.GridSearchCV) for the `cv_results_` attribute.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u-UodWEOedxz"
   },
   "outputs": [],
   "source": [
    "def make_results(model_name:str, model_object, metric:str):\n",
    "    '''\n",
    "    Arguments:\n",
    "        model_name (string): what you want the model to be called in the output table\n",
    "        model_object: a fit GridSearchCV object\n",
    "        metric (string): precision, recall, f1, or accuracy\n",
    "  \n",
    "    Returns a pandas df with the F1, recall, precision, and accuracy scores\n",
    "    for the model with the best mean 'metric' score across all validation folds.  \n",
    "    '''\n",
    "\n",
    "    # Create dictionary that maps input metric to actual metric name in GridSearchCV\n",
    "    metric_dict = {'precision': 'mean_test_precision',\n",
    "                 'recall': 'mean_test_recall',\n",
    "                 'f1': 'mean_test_f1',\n",
    "                 'accuracy': 'mean_test_accuracy',\n",
    "                 }\n",
    "\n",
    "    # Get all the results from the CV and put them in a df\n",
    "    cv_results = pd.DataFrame(model_object.cv_results_)\n",
    "\n",
    "    # Isolate the row of the df with the max(metric) score\n",
    "    best_estimator_results = cv_results.iloc[cv_results[metric_dict[metric]].idxmax(), :]\n",
    "\n",
    "    # Extract Accuracy, precision, recall, and f1 score from that row\n",
    "    f1 = best_estimator_results.mean_test_f1\n",
    "    recall = best_estimator_results.mean_test_recall\n",
    "    precision = best_estimator_results.mean_test_precision\n",
    "    accuracy = best_estimator_results.mean_test_accuracy\n",
    "  \n",
    "    # Create table of results\n",
    "    table = pd.DataFrame()\n",
    "    table = table.append({'Model': model_name,\n",
    "                        'Precision': precision,\n",
    "                        'Recall': recall,\n",
    "                        'F1': f1,\n",
    "                        'Accuracy': accuracy,\n",
    "                        },\n",
    "                        ignore_index=True\n",
    "                       )\n",
    "  \n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cI84Xo37ZLy0"
   },
   "source": [
    "Call `make_results()` on the GridSearch object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qAYb2QigiT_h"
   },
   "outputs": [],
   "source": [
    "results = make_results('random forest 1: precision', rf_cv1, 'precision')\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23gOcTCGp4ix"
   },
   "source": [
    "The precision seems satisfactory, but not great. The other scores are very bad. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SB-yhW9uu7dO"
   },
   "source": [
    "A model with such low F1 and recall scores is not good enough. Try retuning the model to select based on F1 score instead. Consider adjusting the hyperparameters that you try based on the results of the above model. \n",
    "\n",
    "<details>\n",
    "  <summary><h5>HINT</h5></summary>\n",
    "For example, if the available values for `min_samples_split` were [2, 3, 4] and GridSearch identified the best value as 4, consider trying [4, 5, 6] this time.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "ycwjBHJjiT9J"
   },
   "outputs": [],
   "source": [
    "# 1. Instantiate the random forest classifier\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "\n",
    "# 2. Create a dictionary of hyperparameters to tune\n",
    "cv_params = {'max_depth': [5, 6, 7], \n",
    "             'max_features': [1.0],\n",
    "             'max_samples': [0.7],\n",
    "             'min_samples_leaf': [1,2],\n",
    "             'min_samples_split': [2,3],\n",
    "             'n_estimators': [300],\n",
    "             }  \n",
    "\n",
    "# 3. Define a dictionary of scoring metrics to capture\n",
    "scoring = {'accuracy', 'precision', 'recall', 'f1'}\n",
    "\n",
    "# 4. Instantiate the GridSearchCV object\n",
    "rf_cv2 = GridSearchCV(rf, cv_params, scoring=scoring, cv=4, refit='f1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FDRAL7zQx21J"
   },
   "source": [
    "Now fit the model to the `X_train` and `y_train` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iil1LjabiT5x"
   },
   "outputs": [],
   "source": [
    "rf_cv2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4JiP5VRz2un"
   },
   "source": [
    "Get the best score from this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dE6oXEJJiT2R"
   },
   "outputs": [],
   "source": [
    "rf_cv2.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bB-QyGz0RcU"
   },
   "source": [
    "And the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JiLja3YViTzj"
   },
   "outputs": [],
   "source": [
    "rf_cv2.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eTE2QdNP0eEP"
   },
   "source": [
    "Use the `make_results()` function to output all of the scores of your model. Note that it accepts three arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L4TSYXJWiTxs"
   },
   "outputs": [],
   "source": [
    "results = make_results('random forest: f1', rf_cv2, 'f1')\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7m2SOAf2cm4T"
   },
   "source": [
    "There was a modest improvement in both F1 and recall scores, but these results still are not good enough to deploy the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wR1QdIAX1dKX"
   },
   "source": [
    "Use your model to predict on the test data. Assign the results to a variable called `preds`.\n",
    "\n",
    "<details>\n",
    "  <summary><h5>HINT</h5></summary>\n",
    "You cannot call `predict()` on the GridSearchCV object directly. You must call it on the `best_estimator_`.\n",
    "</details>\n",
    "\n",
    "**Note:** For this project, you will use several models to predict on the test data. Remember that this decision comes with a trade-off. What is the benefit of this? What is the drawback?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Y2giCN32Dwc"
   },
   "outputs": [],
   "source": [
    "preds = rf_cv2.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0EnxPK7R1C5Q"
   },
   "source": [
    "Complete the below `get_test_scores()` function you will use to output the scores of the model on the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wc-eIqti1WiQ"
   },
   "outputs": [],
   "source": [
    "def get_test_scores(model_name:str, preds, y_test_data):\n",
    "    '''\n",
    "    Generate a table of test scores.\n",
    "\n",
    "    In: \n",
    "        model_name (string): Your choice: how the model will be named in the output table\n",
    "        preds: numpy array of test predictions\n",
    "        y_test_data: numpy array of y_test data\n",
    "\n",
    "    Out: \n",
    "        table: a pandas df of precision, recall, f1, and accuracy scores for your model\n",
    "    '''\n",
    "    accuracy = round(accuracy_score(y_test_data, preds), 3)\n",
    "    precision = round(precision_score(y_test_data, preds), 3)\n",
    "    recall = round(recall_score(y_test_data, preds), 3)\n",
    "    f1 = round(f1_score(y_test_data, preds), 3)\n",
    "\n",
    "    table = pd.DataFrame({'model': [model_name],\n",
    "                        'precision': [precision], \n",
    "                        'recall': [recall],\n",
    "                        'f1': [f1],\n",
    "                        'accuracy': [accuracy]\n",
    "                        })\n",
    "  \n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cEwnNMMP2Nbb"
   },
   "source": [
    "1. Use the `get_test_scores()` function to generate the scores on the test data. Assign the results to `rf_cv2_test_scores`.\n",
    "2. Call `rf_cv2_test_scores` to output the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g7jShC2TiTvx"
   },
   "outputs": [],
   "source": [
    "rf_cv2_test_scores = get_test_scores('random forest: f1', preds, y_test)\n",
    "rf_cv2_test_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZjClJnncJ-j"
   },
   "source": [
    "**How do your test results compare to your validation results?** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All scores increased by ~0.02."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DOlktJ6l4Tgt"
   },
   "source": [
    "##### **XGBoost**\n",
    "\n",
    " Try to improve your scores using an XGBoost model. \n",
    "\n",
    "1. Instantiate the XGBoost classifier `xgb` and set `objective='binary:logistic'`. Also set the random state.\n",
    "\n",
    "2. Create a dictionary `cv_params` of the following hyperparameters and their corresponding values to tune:\n",
    " - `max_depth`\n",
    " - `min_child_weight`\n",
    " - `learning_rate`\n",
    " - `n_estimators`\n",
    "\n",
    "3. Define a dictionary `scoring` of scoring metrics for grid search to capture (precision, recall, F1 score, and accuracy).\n",
    "\n",
    "4. Instantiate the `GridSearchCV` object `xgb_cv1`. Pass to it as arguments:\n",
    " - estimator=`xgb`\n",
    " - param_grid=`cv_params`\n",
    " - scoring=`scoring`\n",
    " - cv: define the number of you cross-validation folds you want (`cv=_`)\n",
    " - refit: indicate which evaluation metric you want to use to select the model (`refit='f1'`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "0ciO48nhiTqO"
   },
   "outputs": [],
   "source": [
    "# 1. Instantiate the XGBoost classifier\n",
    "xgb = XGBClassifier(objective='binary:logistic', random_state=0)  \n",
    "\n",
    "# 2. Create a dictionary of hyperparameters to tune\n",
    "cv_params = {'max_depth': [4,8,12], \n",
    "             'min_child_weight': [3, 5],\n",
    "             'learning_rate': [0.01, 0.1],\n",
    "             'n_estimators': [300, 500]\n",
    "             }   \n",
    "\n",
    "# 3. Define a dictionary of scoring metrics to capture\n",
    "scoring = {'accuracy', 'precision', 'recall', 'f1'}\n",
    "\n",
    "# 4. Instantiate the GridSearchCV object\n",
    "xgb_cv1 = GridSearchCV(xgb, cv_params, scoring=scoring, cv=4, refit='f1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y78-hQF9680x"
   },
   "source": [
    "Now fit the model to the `X_train` and `y_train` data.\n",
    "\n",
    "**_Note_**: _The following operation may take over an hour to complete_. Therefore, the cell has been commented out along with code cell #50 (where we pickle the model). To save time, you can skip these cells and continue to execute the cells in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dYCWs_HX6804"
   },
   "outputs": [],
   "source": [
    "xgb_cv1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ruQISDB76805"
   },
   "source": [
    "Get the best score from this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UFLTmIDm6805"
   },
   "outputs": [],
   "source": [
    "xgb_cv1.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fwmWDuXZ6805"
   },
   "source": [
    "And the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cdPUCuND6805"
   },
   "outputs": [],
   "source": [
    "xgb_cv1.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8v8HTmQ7KdC"
   },
   "source": [
    "Use the `make_results()` function to output all of the scores of your model. Note that it accepts three arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QL19dH2h7KdD"
   },
   "outputs": [],
   "source": [
    "results = make_results('XGBoost 1: f1', xgb_cv1, 'f1')\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5IRnMO27KdD"
   },
   "source": [
    "Use your model to predict on the test data. Assign the results to a variable called `preds`.\n",
    "\n",
    "<details>\n",
    "  <summary><h5>HINT</h5></summary>\n",
    "You cannot call `predict()` on the GridSearchCV object directly. You must call it on the `best_estimator_`.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nvom_Gu77KdD"
   },
   "outputs": [],
   "source": [
    "preds = xgb_cv1.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IC7FQPA_7KdE"
   },
   "source": [
    "1. Use the `get_test_scores()` function to generate the scores on the test data. Assign the results to `xgb_cv_test_scores`.\n",
    "2. Call `xgb_cv_test_scores` to output the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BIEkS7AA7KdE"
   },
   "outputs": [],
   "source": [
    "xgb_cv_test_scores = get_test_scores('XGBoost 1: f1', preds, y_test)\n",
    "xgb_cv_test_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "saM8YwbAyi-F"
   },
   "source": [
    "**Compare these scores to the random forest test scores. What do you notice? Which model would you choose?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The precision is ~0.02 lower than the random forest model, but recall is over 40% better and F1 is ~24% better. Even accuracy improved. XGBoost is the better model._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eCNH80Ku9TpO"
   },
   "source": [
    "Plot a confusion matrix of the model's predictions on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "5iUyZWjWvqOd"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-652bf4f86bd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Generate array of values for confusion matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb_cv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Plot confusion matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preds' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate array of values for confusion matrix\n",
    "cm = confusion_matrix(y_test, preds, labels=xgb_cv1.classes_)\n",
    "\n",
    "# Plot confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                             display_labels=xgb_cv1.classes_)\n",
    "disp.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dW-3_eWW-k2u"
   },
   "source": [
    "**What type of errors are more common for your model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model is twice as likely to predict a false negative than it is to predict a false positive. Therefore, type II errors are more common. For our use case, this is more desirable, because it's better for a driver to be pleasantly surprised by a generous tip when they weren't expecting one than to be disappointed by a low tip when they were expecting a generous one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XNexnwvy09PK"
   },
   "source": [
    "##### **Feature importance**\n",
    "\n",
    "Use the `plot_importance` function to inspect the top 10 most important features of your final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kz5T1gHc1R2x"
   },
   "outputs": [],
   "source": [
    "plot_importance(xgb_cv1.best_estimator_, max_num_features=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_HGsWfEOeWPm"
   },
   "source": [
    "<img src=\"images/Execute.png\" width=\"100\" height=\"100\" align=left>\n",
    "\n",
    "## PACE: **Execute**\n",
    "\n",
    "Consider the questions in your PACE Strategy Document to reflect on the Execute stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ill21hQ4ej9-"
   },
   "source": [
    "### **Task 4. Conclusion**\n",
    "\n",
    "In this step, use the results of the models above to formulate a conclusion. Consider the following questions:\n",
    "\n",
    "1. **Would you recommend using this model? Why or why not?**  \n",
    "\n",
    "2. **What was your model doing? Can you explain how it was making predictions?**   \n",
    "\n",
    "3. **Are there new features that you can engineer that might improve model performance?**   \n",
    "\n",
    "4. **What features would you want to have that would likely improve the performance of your model?**   \n",
    "\n",
    "Remember, sometimes your data simply will not be predictive of your chosen target. This is common. Machine learning is a powerful tool, but it is not magic. If your data does not contain predictive signal, even the most complex algorithm will not be able to deliver consistent and accurate predictions. Do not be afraid to draw this conclusion. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [
    {
     "file_id": "1oNheYh5WbljxkvoK_BMkQTey2DWnFXMs",
     "timestamp": 1663785370813
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
